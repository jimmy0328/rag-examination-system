import os
import json
from typing import List, Dict, Any, Optional
from sentence_transformers import SentenceTransformer
from pinecone import Pinecone
import google.generativeai as genai
import time

class RAGSystem:
    """
    RAG (Retrieval-Augmented Generation) ç³»çµ±
    çµåˆ Pinecone å‘é‡æœå°‹å’Œ Gemini LLM ç”Ÿæˆå›ç­”
    """
    
    def __init__(self, 
                 pinecone_api_key: str,
                 gemini_api_key: str,
                 pinecone_env: str = "us-east-1",
                 index_name: str = "text-chunks-index"):
        """
        åˆå§‹åŒ– RAG ç³»çµ±
        
        Args:
            pinecone_api_key: Pinecone API é‡‘é‘°
            gemini_api_key: Gemini API é‡‘é‘°
            pinecone_env: Pinecone ç’°å¢ƒ
            index_name: Pinecone ç´¢å¼•åç¨±
        """
        self.pinecone_api_key = pinecone_api_key
        self.gemini_api_key = gemini_api_key
        self.pinecone_env = pinecone_env
        self.index_name = index_name
        
        # åˆå§‹åŒ–çµ„ä»¶
        self._initialize_pinecone()
        self._initialize_gemini()
        self._initialize_embedding_model()
    
    def _initialize_pinecone(self):
        """åˆå§‹åŒ– Pinecone å®¢æˆ¶ç«¯"""
        try:
            self.pc = Pinecone(api_key=self.pinecone_api_key)
            self.index = self.pc.Index(self.index_name)
            print(f"âœ… Pinecone åˆå§‹åŒ–æˆåŠŸï¼Œé€£æ¥åˆ°ç´¢å¼•: {self.index_name}")
        except Exception as e:
            print(f"âŒ Pinecone åˆå§‹åŒ–å¤±æ•—: {str(e)}")
            raise
    
    def _initialize_gemini(self):
        """åˆå§‹åŒ– Gemini LLM"""
        try:
            genai.configure(api_key=self.gemini_api_key)
            # ä½¿ç”¨å…è²»çš„ gemini-1.5-flash æ¨¡å‹
            self.model = genai.GenerativeModel('gemini-2.0-flash')
            print("âœ… Gemini LLM åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âŒ Gemini åˆå§‹åŒ–å¤±æ•—: {str(e)}")
            raise
    
    def _initialize_embedding_model(self):
        """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹"""
        try:
            print("æ­£åœ¨è¼‰å…¥åµŒå…¥æ¨¡å‹...")
            self.embedding_model = SentenceTransformer('shibing624/text2vec-base-chinese')
            print("âœ… åµŒå…¥æ¨¡å‹è¼‰å…¥å®Œæˆ")
        except Exception as e:
            print(f"âŒ åµŒå…¥æ¨¡å‹è¼‰å…¥å¤±æ•—: {str(e)}")
            raise
    
    def retrieve_similar_chunks(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:
        """
        å¾ Pinecone æª¢ç´¢æœ€ç›¸ä¼¼çš„æ–‡å­—å¡Š
        
        Args:
            query: æŸ¥è©¢æ–‡å­—
            top_k: æª¢ç´¢çš„æ–‡å­—å¡Šæ•¸é‡
        
        Returns:
            ç›¸ä¼¼æ–‡å­—å¡Šåˆ—è¡¨
        """
        try:
            # ç”ŸæˆæŸ¥è©¢å‘é‡
            query_embedding = self.embedding_model.encode([query]).tolist()[0]
            
            # åŸ·è¡Œå‘é‡æœå°‹
            results = self.index.query(
                vector=query_embedding,
                top_k=top_k,
                include_metadata=True
            )
            
            # æå–ç›¸é—œä¿¡æ¯
            retrieved_chunks = []
            for match in results['matches']:
                chunk_info = {
                    'id': match['id'],
                    'score': match['score'],
                    'text': match['metadata'].get('text', ''),
                    'source_file': match['metadata'].get('source_file', 'Unknown'),
                    'chunk_index': match['metadata'].get('chunk_index', -1),
                    'metadata': match['metadata']
                }
                retrieved_chunks.append(chunk_info)
            
            print(f"âœ… æˆåŠŸæª¢ç´¢åˆ° {len(retrieved_chunks)} å€‹ç›¸é—œæ–‡å­—å¡Š")
            return retrieved_chunks
            
        except Exception as e:
            print(f"âŒ æª¢ç´¢éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return []
    
    def format_context(self, chunks: List[Dict[str, Any]]) -> str:
        """
        æ ¼å¼åŒ–æª¢ç´¢åˆ°çš„æ–‡å­—å¡Šä½œç‚ºä¸Šä¸‹æ–‡
        
        Args:
            chunks: æª¢ç´¢åˆ°çš„æ–‡å­—å¡Šåˆ—è¡¨
        
        Returns:
            æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡å­—ä¸²
        """
        if not chunks:
            return "æ²’æœ‰æ‰¾åˆ°ç›¸é—œçš„ä¸Šä¸‹æ–‡è³‡è¨Šã€‚"
        
        context_parts = []
        for i, chunk in enumerate(chunks, 1):
            context_part = f"""
=== ç›¸é—œè³‡è¨Š {i} (ç›¸ä¼¼åº¦: {chunk['score']:.4f}) ===
ä¾†æº: {chunk['source_file']}
å…§å®¹: {chunk['text']}
"""
            context_parts.append(context_part)
        
        return "\n".join(context_parts)
    
    def generate_prompt(self, query: str, context: str) -> str:
        """
        ç”Ÿæˆç™¼é€çµ¦ Gemini çš„æç¤ºè©
        
        Args:
            query: ç”¨æˆ¶æŸ¥è©¢
            context: æª¢ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
        
        Returns:
            å®Œæ•´çš„æç¤ºè©
        """
        prompt = f"""
ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ AI åŠ©æ‰‹ã€‚è«‹æ ¹æ“šä»¥ä¸‹æä¾›çš„ä¸Šä¸‹æ–‡è³‡è¨Šä¾†å›ç­”ç”¨æˆ¶çš„å•é¡Œã€‚

<context>
{context}
</context>

ç”¨æˆ¶å•é¡Œ: {query}

è«‹æ ¹æ“šä¸Šè¿°ä¸Šä¸‹æ–‡è³‡è¨Šå›ç­”å•é¡Œã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²’æœ‰è¶³å¤ çš„è³‡è¨Šä¾†å›ç­”å•é¡Œï¼Œè«‹èª å¯¦åœ°èªªæ˜ï¼Œä¸¦åŸºæ–¼ä½ çš„ä¸€èˆ¬çŸ¥è­˜æä¾›æœ‰å¹«åŠ©çš„å›ç­”ã€‚

å›ç­”è¦æ±‚ï¼š
1. åŸºæ–¼æä¾›çš„ä¸Šä¸‹æ–‡è³‡è¨Šé€²è¡Œå›ç­”
2. å¦‚æœä¸Šä¸‹æ–‡ç›¸é—œï¼Œè«‹å¼•ç”¨ç›¸é—œå…§å®¹
3. å›ç­”è¦æº–ç¢ºã€è©³ç´°ä¸”æœ‰å¹«åŠ©
4. ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”
"""
        return prompt
    
    def query_gemini(self, prompt: str, max_retries: int = 3) -> str:
        """
        å‘ Gemini ç™¼é€æŸ¥è©¢ä¸¦ç²å–å›ç­”
        
        Args:
            prompt: å®Œæ•´çš„æç¤ºè©
            max_retries: æœ€å¤§é‡è©¦æ¬¡æ•¸
        
        Returns:
            Gemini çš„å›ç­”
        """
        for attempt in range(max_retries):
            try:
                response = self.model.generate_content(prompt)
                
                if response.text:
                    return response.text
                else:
                    print(f"âš ï¸ Gemini å›æ‡‰ç‚ºç©ºï¼Œå˜—è©¦ {attempt + 1}/{max_retries}")
                    
            except Exception as e:
                print(f"âŒ Gemini æŸ¥è©¢å¤±æ•— (å˜—è©¦ {attempt + 1}/{max_retries}): {str(e)}")
                if attempt < max_retries - 1:
                    time.sleep(2)  # ç­‰å¾…2ç§’å¾Œé‡è©¦
                    
        return "æŠ±æ­‰ï¼Œç„¡æ³•å¾ Gemini ç²å–å›ç­”ã€‚è«‹ç¨å¾Œå†è©¦ã€‚"
    
    def query(self, query: str, top_k: int = 3, similarity_threshold: float = 0.5) -> Dict[str, Any]:
        """
        åŸ·è¡Œå®Œæ•´çš„ RAG æŸ¥è©¢æµç¨‹
        
        Args:
            query: ç”¨æˆ¶æŸ¥è©¢
            top_k: æª¢ç´¢çš„æ–‡å­—å¡Šæ•¸é‡
            similarity_threshold: ç›¸ä¼¼åº¦é–¾å€¼ï¼Œä½æ–¼æ­¤å€¼è¦–ç‚ºä¸ç›¸é—œ
        
        Returns:
            åŒ…å«æª¢ç´¢çµæœå’Œ LLM å›ç­”çš„å­—å…¸
        """
        print(f"ğŸ” é–‹å§‹ RAG æŸ¥è©¢: {query}")
        
        # 1. æª¢ç´¢ç›¸é—œæ–‡å­—å¡Š
        retrieved_chunks = self.retrieve_similar_chunks(query, top_k)
        
        # 2. æª¢æŸ¥æ˜¯å¦æœ‰ç›¸é—œå…§å®¹
        if not retrieved_chunks:
            return {
                'query': query,
                'retrieved_chunks': [],
                'context': "æ²’æœ‰æ‰¾åˆ°ç›¸é—œçš„ä¸Šä¸‹æ–‡è³‡è¨Šã€‚",
                'answer': "æŠ±æ­‰ï¼Œæ‚¨æŸ¥è©¢çš„å…§å®¹ä¸åœ¨æˆ‘å€‘çš„è³‡æ–™åº«ä¸­ã€‚è«‹å˜—è©¦ä½¿ç”¨å…¶ä»–é—œéµå­—æˆ–é‡æ–°æè¿°æ‚¨çš„å•é¡Œã€‚",
                'success': False,
                'has_context': False
            }
        
        # 3. æª¢æŸ¥æœ€é«˜ç›¸ä¼¼åº¦æ˜¯å¦é”åˆ°é–¾å€¼
        max_similarity = max(chunk['score'] for chunk in retrieved_chunks)
        print(f"ğŸ“Š æœ€é«˜ç›¸ä¼¼åº¦: {max_similarity:.4f} (é–¾å€¼: {similarity_threshold})")
        
        if max_similarity < similarity_threshold:
            return {
                'query': query,
                'retrieved_chunks': retrieved_chunks,
                'context': "æª¢ç´¢åˆ°çš„å…§å®¹ç›¸ä¼¼åº¦éä½ï¼Œå¯èƒ½èˆ‡æŸ¥è©¢ä¸ç›¸é—œã€‚",
                'answer': f"æŠ±æ­‰ï¼Œæ‚¨æŸ¥è©¢çš„å…§å®¹åœ¨æˆ‘å€‘çš„è³‡æ–™åº«ä¸­æ²’æœ‰æ‰¾åˆ°è¶³å¤ ç›¸é—œçš„è³‡è¨Šï¼ˆæœ€é«˜ç›¸ä¼¼åº¦: {max_similarity:.2f}ï¼‰ã€‚è«‹å˜—è©¦ä½¿ç”¨å…¶ä»–é—œéµå­—æˆ–é‡æ–°æè¿°æ‚¨çš„å•é¡Œã€‚",
                'success': False,
                'has_context': False
            }
        
        # 4. æ ¼å¼åŒ–ä¸Šä¸‹æ–‡
        context = self.format_context(retrieved_chunks)
        
        # 5. ç”Ÿæˆæç¤ºè©
        prompt = self.generate_prompt(query, context)
        
        # 6. æŸ¥è©¢ Gemini LLM
        answer = self.query_gemini(prompt)
        
        # 7. æ•´ç†çµæœ
        result = {
            'query': query,
            'retrieved_chunks': retrieved_chunks,
            'context': context,
            'answer': answer,
            'success': True,
            'has_context': True
        }
        
        return result 