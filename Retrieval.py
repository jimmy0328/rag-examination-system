# RAGæª¢ç´¢èˆ‡GeminiæŸ¥è©¢ç³»çµ±
# ç”¨æ–¼å¾Pineconeå‘é‡è³‡æ–™åº«æª¢ç´¢ç›¸é—œå…§å®¹ä¸¦ä½¿ç”¨Gemini LLMç”Ÿæˆå›ç­”

import os
import json
from typing import List, Dict, Any, Optional
from sentence_transformers import SentenceTransformer
from pinecone import Pinecone
import google.generativeai as genai
import time

class RAGRetriever:
    """
    RAG (Retrieval-Augmented Generation) æª¢ç´¢å™¨
    çµåˆPineconeå‘é‡æœå°‹å’ŒGemini LLMç”Ÿæˆå›ç­”
    """
    
    def __init__(self, 
                 pinecone_api_key: str,
                 gemini_api_key: str,
                 pinecone_env: str = "us-east-1",
                 index_name: str = "text-chunks-index"):
        """
        åˆå§‹åŒ–RAGæª¢ç´¢å™¨
        
        Args:
            pinecone_api_key: Pinecone APIé‡‘é‘°
            gemini_api_key: Gemini APIé‡‘é‘°
            pinecone_env: Pineconeç’°å¢ƒ
            index_name: Pineconeç´¢å¼•åç¨±
        """
        self.pinecone_api_key = pinecone_api_key
        self.gemini_api_key = gemini_api_key
        self.pinecone_env = pinecone_env
        self.index_name = index_name
        
        # åˆå§‹åŒ–çµ„ä»¶
        self._initialize_pinecone()
        self._initialize_gemini()
        self._initialize_embedding_model()
    
    def _initialize_pinecone(self):
        """åˆå§‹åŒ–Pineconeå®¢æˆ¶ç«¯"""
        try:
            self.pc = Pinecone(api_key=self.pinecone_api_key)
            self.index = self.pc.Index(self.index_name)
            print(f"âœ… Pineconeåˆå§‹åŒ–æˆåŠŸï¼Œé€£æ¥åˆ°ç´¢å¼•: {self.index_name}")
        except Exception as e:
            print(f"âŒ Pineconeåˆå§‹åŒ–å¤±æ•—: {str(e)}")
            raise
    
    def _initialize_gemini(self):
        """åˆå§‹åŒ–Gemini LLM"""
        try:
            genai.configure(api_key=self.gemini_api_key)
            # ä½¿ç”¨å…è²»çš„gemini-1.5-flashæ¨¡å‹
            self.model = genai.GenerativeModel('gemini-1.5-flash')
            print("âœ… Gemini LLMåˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âŒ Geminiåˆå§‹åŒ–å¤±æ•—: {str(e)}")
            raise
    
    def _initialize_embedding_model(self):
        """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹"""
        try:
            print("æ­£åœ¨è¼‰å…¥åµŒå…¥æ¨¡å‹...")
            self.embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
            print("âœ… åµŒå…¥æ¨¡å‹è¼‰å…¥å®Œæˆ")
        except Exception as e:
            print(f"âŒ åµŒå…¥æ¨¡å‹è¼‰å…¥å¤±æ•—: {str(e)}")
            raise
    
    def retrieve_similar_chunks(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:
        """
        å¾Pineconeæª¢ç´¢æœ€ç›¸ä¼¼çš„æ–‡å­—å¡Š
        
        Args:
            query: æŸ¥è©¢æ–‡å­—
            top_k: æª¢ç´¢çš„æ–‡å­—å¡Šæ•¸é‡
        
        Returns:
            ç›¸ä¼¼æ–‡å­—å¡Šåˆ—è¡¨
        """
        try:
            # ç”ŸæˆæŸ¥è©¢å‘é‡
            query_embedding = self.embedding_model.encode([query]).tolist()[0]
            
            # åŸ·è¡Œå‘é‡æœå°‹
            results = self.index.query(
                vector=query_embedding,
                top_k=top_k,
                include_metadata=True
            )
            
            # æå–ç›¸é—œä¿¡æ¯
            retrieved_chunks = []
            for match in results['matches']:
                chunk_info = {
                    'id': match['id'],
                    'score': match['score'],
                    'text': match['metadata'].get('text', ''),
                    'source_file': match['metadata'].get('source_file', 'Unknown'),
                    'chunk_index': match['metadata'].get('chunk_index', -1),
                    'metadata': match['metadata']
                }
                retrieved_chunks.append(chunk_info)
            
            print(f"âœ… æˆåŠŸæª¢ç´¢åˆ° {len(retrieved_chunks)} å€‹ç›¸é—œæ–‡å­—å¡Š")
            return retrieved_chunks
            
        except Exception as e:
            print(f"âŒ æª¢ç´¢éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return []
    
    def format_context(self, chunks: List[Dict[str, Any]]) -> str:
        """
        æ ¼å¼åŒ–æª¢ç´¢åˆ°çš„æ–‡å­—å¡Šä½œç‚ºä¸Šä¸‹æ–‡
        
        Args:
            chunks: æª¢ç´¢åˆ°çš„æ–‡å­—å¡Šåˆ—è¡¨
        
        Returns:
            æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡å­—ä¸²
        """
        if not chunks:
            return "æ²’æœ‰æ‰¾åˆ°ç›¸é—œçš„ä¸Šä¸‹æ–‡è³‡è¨Šã€‚"
        
        context_parts = []
        for i, chunk in enumerate(chunks, 1):
            context_part = f"""
=== ç›¸é—œè³‡è¨Š {i} (ç›¸ä¼¼åº¦: {chunk['score']:.4f}) ===
ä¾†æº: {chunk['source_file']}
å…§å®¹: {chunk['text']}
"""
            context_parts.append(context_part)
        
        return "\n".join(context_parts)
    
    def generate_prompt(self, query: str, context: str) -> str:
        """
        ç”Ÿæˆç™¼é€çµ¦Geminiçš„æç¤ºè©
        
        Args:
            query: ç”¨æˆ¶æŸ¥è©¢
            context: æª¢ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
        
        Returns:
            å®Œæ•´çš„æç¤ºè©
        """
        prompt = f"""
ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„AIåŠ©æ‰‹ã€‚è«‹æ ¹æ“šä»¥ä¸‹æä¾›çš„ä¸Šä¸‹æ–‡è³‡è¨Šä¾†å›ç­”ç”¨æˆ¶çš„å•é¡Œã€‚

<context>
{context}
</context>

ç”¨æˆ¶å•é¡Œ: {query}

è«‹æ ¹æ“šä¸Šè¿°ä¸Šä¸‹æ–‡è³‡è¨Šå›ç­”å•é¡Œã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²’æœ‰è¶³å¤ çš„è³‡è¨Šä¾†å›ç­”å•é¡Œï¼Œè«‹èª å¯¦åœ°èªªæ˜ï¼Œä¸¦åŸºæ–¼ä½ çš„ä¸€èˆ¬çŸ¥è­˜æä¾›æœ‰å¹«åŠ©çš„å›ç­”ã€‚

å›ç­”è¦æ±‚ï¼š
1. åŸºæ–¼æä¾›çš„ä¸Šä¸‹æ–‡è³‡è¨Šé€²è¡Œå›ç­”
2. å¦‚æœä¸Šä¸‹æ–‡ç›¸é—œï¼Œè«‹å¼•ç”¨ç›¸é—œå…§å®¹
3. å›ç­”è¦æº–ç¢ºã€è©³ç´°ä¸”æœ‰å¹«åŠ©
4. ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”
"""
        return prompt
    
    def query_gemini(self, prompt: str, max_retries: int = 3) -> str:
        """
        å‘Geminiç™¼é€æŸ¥è©¢ä¸¦ç²å–å›ç­”
        
        Args:
            prompt: å®Œæ•´çš„æç¤ºè©
            max_retries: æœ€å¤§é‡è©¦æ¬¡æ•¸
        
        Returns:
            Geminiçš„å›ç­”
        """
        for attempt in range(max_retries):
            try:
                print(prompt)
                response = self.model.generate_content(prompt)
                
                if response.text:
                    return response.text
                else:
                    print(f"âš ï¸ Geminiå›æ‡‰ç‚ºç©ºï¼Œå˜—è©¦ {attempt + 1}/{max_retries}")
                    
            except Exception as e:
                print(f"âŒ GeminiæŸ¥è©¢å¤±æ•— (å˜—è©¦ {attempt + 1}/{max_retries}): {str(e)}")
                if attempt < max_retries - 1:
                    time.sleep(2)  # ç­‰å¾…2ç§’å¾Œé‡è©¦
                    
        return "æŠ±æ­‰ï¼Œç„¡æ³•å¾Geminiç²å–å›ç­”ã€‚è«‹ç¨å¾Œå†è©¦ã€‚"
    
    def rag_query(self, query: str, top_k: int = 3, verbose: bool = True) -> Dict[str, Any]:
        """
        åŸ·è¡Œå®Œæ•´çš„RAGæŸ¥è©¢æµç¨‹
        
        Args:
            query: ç”¨æˆ¶æŸ¥è©¢
            top_k: æª¢ç´¢çš„æ–‡å­—å¡Šæ•¸é‡
            verbose: æ˜¯å¦é¡¯ç¤ºè©³ç´°éç¨‹
        
        Returns:
            åŒ…å«æª¢ç´¢çµæœå’ŒLLMå›ç­”çš„å­—å…¸
        """
        if verbose:
            print("=" * 60)
            print(f"ğŸ” é–‹å§‹RAGæŸ¥è©¢: {query}")
            print("=" * 60)
        
        # 1. æª¢ç´¢ç›¸é—œæ–‡å­—å¡Š
        if verbose:
            print("ğŸ“– æ­£åœ¨æª¢ç´¢ç›¸é—œæ–‡å­—å¡Š...")
        retrieved_chunks = self.retrieve_similar_chunks(query, top_k)
        
        if not retrieved_chunks:
            return {
                'query': query,
                'retrieved_chunks': [],
                'context': "æ²’æœ‰æ‰¾åˆ°ç›¸é—œçš„ä¸Šä¸‹æ–‡è³‡è¨Šã€‚",
                'answer': "æŠ±æ­‰ï¼Œæ²’æœ‰æ‰¾åˆ°ç›¸é—œçš„è³‡è¨Šä¾†å›ç­”æ‚¨çš„å•é¡Œã€‚",
                'success': False
            }
        
        # 2. æ ¼å¼åŒ–ä¸Šä¸‹æ–‡
        context = self.format_context(retrieved_chunks)
        
        if verbose:
            print("ğŸ“‹ æª¢ç´¢åˆ°çš„ä¸Šä¸‹æ–‡:")
            print("-" * 40)
            for i, chunk in enumerate(retrieved_chunks, 1):
                print(f"{i}. ç›¸ä¼¼åº¦: {chunk['score']:.4f}")
                print(f"   ä¾†æº: {chunk['source_file']}")
                print(f"   å…§å®¹é è¦½: {chunk['text'][:100]}...")
                print()
        
        # 3. ç”Ÿæˆæç¤ºè©
        prompt = self.generate_prompt(query, context)
        
        # 4. æŸ¥è©¢Gemini LLM
        if verbose:
            print("ğŸ¤– æ­£åœ¨å‘Gemini LLMç™¼é€æŸ¥è©¢...")
        answer = self.query_gemini(prompt)
        
        # 5. æ•´ç†çµæœ
        result = {
            'query': query,
            'retrieved_chunks': retrieved_chunks,
            'context': context,
            'answer': answer,
            'success': True
        }
        
        if verbose:
            print("=" * 60)
            print("ğŸ¯ æœ€çµ‚å›ç­”:")
            print("=" * 60)
            print(answer)
            print("=" * 60)
        
        return result
    
    def batch_query(self, queries: List[str], top_k: int = 3) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡åŸ·è¡ŒRAGæŸ¥è©¢
        
        Args:
            queries: æŸ¥è©¢åˆ—è¡¨
            top_k: æ¯å€‹æŸ¥è©¢æª¢ç´¢çš„æ–‡å­—å¡Šæ•¸é‡
        
        Returns:
            æŸ¥è©¢çµæœåˆ—è¡¨
        """
        results = []
        for i, query in enumerate(queries, 1):
            print(f"\nè™•ç†æŸ¥è©¢ {i}/{len(queries)}: {query}")
            result = self.rag_query(query, top_k, verbose=False)
            results.append(result)
            time.sleep(1)  # é¿å…APIé€Ÿç‡é™åˆ¶
        
        return results
    
    def save_results(self, results: List[Dict[str, Any]], filename: str = "rag_results.json"):
        """
        å„²å­˜æŸ¥è©¢çµæœåˆ°JSONæª”æ¡ˆ
        
        Args:
            results: æŸ¥è©¢çµæœåˆ—è¡¨
            filename: å„²å­˜æª”æ¡ˆå
        """
        try:
            # æ¸…ç†çµæœä»¥ä¾¿JSONåºåˆ—åŒ–
            clean_results = []
            for result in results:
                clean_result = {
                    'query': result['query'],
                    'answer': result['answer'],
                    'retrieved_chunks': [
                        {
                            'score': chunk['score'],
                            'text': chunk['text'],
                            'source_file': chunk['source_file']
                        }
                        for chunk in result['retrieved_chunks']
                    ],
                    'success': result['success']
                }
                clean_results.append(clean_result)
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(clean_results, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… çµæœå·²å„²å­˜åˆ° {filename}")
            
        except Exception as e:
            print(f"âŒ å„²å­˜çµæœæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")

def main():
    """ä¸»è¦åŸ·è¡Œå‡½å¼"""
    # APIé‡‘é‘°é…ç½®
    PINECONE_API_KEY = "PineconeAPIKey"
    GEMINI_API_KEY = "GeminiAPIKey"
    
    # åˆå§‹åŒ–RAGæª¢ç´¢å™¨
    try:
        rag = RAGRetriever(
            pinecone_api_key=PINECONE_API_KEY,
            gemini_api_key=GEMINI_API_KEY,
            pinecone_env="us-east-1",
            index_name="text-chunks-index"
        )
        
        print("ğŸš€ RAGç³»çµ±åˆå§‹åŒ–å®Œæˆï¼")
        print("=" * 60)
        
    except Exception as e:
        print(f"âŒ RAGç³»çµ±åˆå§‹åŒ–å¤±æ•—: {str(e)}")
        return
    
    # äº’å‹•å¼æŸ¥è©¢æ¨¡å¼
    print("ğŸ’¬ é€²å…¥äº’å‹•å¼æŸ¥è©¢æ¨¡å¼ (è¼¸å…¥ 'quit' é€€å‡º)")
    print("=" * 60)
    
    while True:
        try:
            query = input("\nè«‹è¼¸å…¥æ‚¨çš„å•é¡Œ: ").strip()
            
            if query.lower() in ['quit', 'exit', 'é€€å‡º']:
                print("ğŸ‘‹ æ„Ÿè¬ä½¿ç”¨RAGæŸ¥è©¢ç³»çµ±ï¼")
                break
            
            if not query:
                print("âš ï¸ è«‹è¼¸å…¥æœ‰æ•ˆçš„å•é¡Œ")
                continue
            
            # åŸ·è¡ŒRAGæŸ¥è©¢
            result = rag.rag_query(query, top_k=3, verbose=True)
            
            # è©¢å•æ˜¯å¦å„²å­˜çµæœ
            save_choice = input("\næ˜¯å¦è¦å„²å­˜æ­¤æ¬¡æŸ¥è©¢çµæœï¼Ÿ(y/n): ").strip().lower()
            if save_choice in ['y', 'yes', 'æ˜¯']:
                timestamp = time.strftime("%Y%m%d_%H%M%S")
                filename = f"rag_result_{timestamp}.json"
                rag.save_results([result], filename)
                
        except KeyboardInterrupt:
            print("\nğŸ‘‹ ç¨‹å¼å·²ä¸­æ–·ï¼Œæ„Ÿè¬ä½¿ç”¨ï¼")
            break
        except Exception as e:
            print(f"âŒ æŸ¥è©¢éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")

# ä½¿ç”¨ç¯„ä¾‹
def example_usage():
    """ä½¿ç”¨ç¯„ä¾‹"""
    # APIé‡‘é‘°é…ç½®
    PINECONE_API_KEY = "PineconeAPIKey"
    GEMINI_API_KEY = "GeminiAPIKey"
    
    # åˆå§‹åŒ–RAGæª¢ç´¢å™¨
    rag = RAGRetriever(
        pinecone_api_key=PINECONE_API_KEY,
        gemini_api_key=GEMINI_API_KEY
    )
    
    # å–®ä¸€æŸ¥è©¢ç¯„ä¾‹
    query = "ä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’ï¼Ÿ"
    result = rag.rag_query(query, top_k=3, verbose=True)
    
    # æ‰¹é‡æŸ¥è©¢ç¯„ä¾‹
    queries = [
        "ä»€éº¼æ˜¯æ·±åº¦å­¸ç¿’ï¼Ÿ",
        "å‘é‡è³‡æ–™åº«çš„ä½œç”¨æ˜¯ä»€éº¼ï¼Ÿ",
        "äººå·¥æ™ºæ…§çš„ç™¼å±•è¶¨å‹¢å¦‚ä½•ï¼Ÿ"
    ]
    
    batch_results = rag.batch_query(queries, top_k=3)
    rag.save_results(batch_results, "batch_results.json")

if __name__ == "__main__":
    # åŸ·è¡Œä¸»ç¨‹å¼ (äº’å‹•å¼æ¨¡å¼)
    main()
    
    # æˆ–è€…åŸ·è¡Œä½¿ç”¨ç¯„ä¾‹
    # example_usage()